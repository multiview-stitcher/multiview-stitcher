{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"multiview-stitcher","text":"<p><code>multiview-stitcher</code> is an open-source modular toolbox for distributed and tiled stitching of 2-3D image data in python. It is an extensible framework including a collection of algorithms to register and fuse small and large datasets from multi-positioning and multi-view light sheet microscopy, as well as other modalities such as correlative cryo-EM datasets.</p> <p>For visualization, the associated <code>napari-stitcher</code> provides visualization functionality using the Napari viewer, including a standalone widget.</p> <p>With a focus on interoperability and integration with existing tools and the ecosystem, the package intends to integrate as tightly as possible with the NGFF specification.</p> <p>It leverages <code>xarray</code> in combination with <code>spatial-image</code> classes for image metadata handling and <code>dask</code> (and <code>dask-image</code>) for chunked and distributed image processing.</p> <p><code>multiview-stitcher</code> as a modular framework for registration and fusion</p> <p>While <code>multiview-stitcher</code> contains a set of built-in functions for stitching, it is also possible to extend the package with custom functions. This can be useful for adding new registration algorithms, fusion methods, or other functionality. Have a look at the extension API documentation for more information.</p>"},{"location":"#napari-plugin","title":"Napari plugin","text":"<p>There's an associated napari plugin: napari-stitcher.</p>"},{"location":"#previous-work","title":"Previous work","text":"<p><code>multiview-stitcher</code> improves and replaces MVRegFus.</p> <p>Note</p> <p><code>multiview-stitcher</code> is a work in progress. The API is not yet stable.</p>"},{"location":"chart/","title":"Workflow chart","text":""},{"location":"chart/#higher-level","title":"Higher-level","text":"<pre><code>%%{init: { \"graph\": { \"htmlLabels\": true, \"curve\": \"linear\" } } }%%\ngraph TD\n%%   A[Numpy-like arrays] --&gt; |Define geometry| B{Tiles};\n  Tiles@{ shape: procs, label: \"Tiles\"}\n\n  Tile_i@{ shape: tag-rect, label: \"Tile i\" }\n  TileN@{ shape: brace-r, label: \"further tiles...\" }\n\n  ArrayNP@{ shape: win-pane, label: \"Numpy\" }\n  ArrayDA@{ shape: win-pane, label: \"Dask\" }\n  ArrayCP@{ shape: win-pane, label: \"CuPy\" }\n\n  Scale@{ shape: notch-rect, label: \"Scale\\n Translation\" }\n  Dimensions@{ shape: notch-rect, label: \"Image dimensions\\n(C,) (T,) (Z,) Y, X\" }\n\n  Fused@{ shape: tag-rect, label: \"Fused image\" }\n  Fuse@{ shape: rounded, label: \"Fuse tiles\"}\n\n  GraphConst@{ shape: rounded, label: \"Construct graph\" }\n  PairReg@{ shape: rounded, label: \"Pairwise\\n view registration\" }\n  GloRes@{ shape: rounded, label: \"Global parameter\\n resolution\" }\n  Params@{ shape: procs, label: \"Transform parameters\"}\n\n  Storage@{ shape: lin-cyl, label: \"Storage\" }\n\n  Tile_i --&gt; Tiles\n%%   Tile2 --&gt; Tiles\n  TileN --&gt; Tiles\n\n  Tiles --&gt; |Select initial transform_key| GraphConst\n  Params --&gt; |Set new transform_key| Tiles\n\n  Tiles --&gt; |Select transform_key| Fuse\n\n  subgraph Input tile definition\n    subgraph Input-arrays\n        ArrayNP\n        ArrayDA\n        ArrayCP\n    end\n    Input-arrays --&gt; Tile_i\n    Scale --&gt; |Defines default transform_key| Tile_i\n    Dimensions --&gt; Tile_i\n    %% Array2 --&gt; Tile1\n    %% Scale2 --&gt; Tile2\n    TileN\n  end\n\n  subgraph Registration\n  GraphConst --&gt; PairReg\n  PairReg --&gt; GloRes\n  GloRes --&gt; Params\n  end\n\n  subgraph Fusion\n    Fuse --&gt; Fused\n    Fused --&gt; Storage\n  end\n</code></pre>"},{"location":"chart/#lower-level","title":"Lower-level","text":"<pre><code>graph TD\n%%   A[Numpy-like arrays] --&gt; |Define geometry| B{Tiles};\n  Tiles@{ shape: procs, label: \"Tiles\"}\n\n  Tile_i@{ shape: tag-rect, label: \"Tile i\" }\n  TileN@{ shape: brace-r, label: \"further tiles...\" }\n\n  ArrayNP@{ shape: win-pane, label: \"Numpy\" }\n  ArrayDA@{ shape: win-pane, label: \"Dask\" }\n  ArrayCP@{ shape: win-pane, label: \"CuPy\" }\n\n  Scale@{ shape: notch-rect, label: \"Scale Translation\" }\n  Dimensions@{ shape: notch-rect, label: \"Image dimensions\\n(C,) (T,) (Z,) Y, X\" }\n\n  Fused@{ shape: tag-rect, label: \"Fused image\" }\n  Fuse@{ shape: circle, label: \"Fuse tiles\"}\n\n%%   Fusion-methods@{ shape: brace-r, label: \"**Fusion methods**\n%%     - weighted average\n%%     - multi-view deconvolution\n%%     - custom API\n%% \" }\n\n  Weighted_average@{ shape: rounded, label: \"Weighted average\" }\n  Max_fusion@{ shape: rounded, label: \"Maximum intensity\" }\n  Multi-view-deconvolution@{ shape: rounded, label: \"Multi-view deconvolution\" }\n  Custom-fusion@{ shape: rounded, label: \"Custom API\" }\n\n  Blending_weights@{ shape: rect, label: \"Linear blending\" }\n  Content_weights@{ shape: rect, label: \"Content-based\" }\n  Custom_weights@{ shape: rect, label: \"Custom API\" }\n\n  subgraph Graph-construction[\"View graph construction\"]\n  direction TB\n    Tile1 --&gt; |overlaps| Tile2 &amp; Tile3--&gt; |overlaps| Tile4\n  end\n\n  subgraph Pairwise-registration\n    direction RL\n    RegPhase@{ shape: rounded, label: \"Phase correlation\" }\n    RegAnts@{ shape: rounded, label: \"AntsPy\" }\n    RegElastix@{ shape: rounded, label: \"ITKElastix\" }\n    RegCustom@{ shape: rounded, label: \"Custom API\" }\n  end\n\n  subgraph Global-resolution\n    direction RL\n    GloRes_shortest@{ shape: rounded, label: \"Shortest Paths\" }\n    GloRes_globalopt@{ shape: rounded, label: \"Global Optimization\" }\n    GloRes_custom@{ shape: rounded, label: \"Custom API\" }\n  end\n\n  Params@{ shape: procs, label: \"Transform parameters\"}\n\n  Storage@{ shape: lin-cyl, label: \"Storage\" }\n\n  Tile_i --&gt; Tiles\n  TileN --&gt; Tiles\n\n  Tiles --&gt; |Select initial transform_key| Graph-construction\n  Params --&gt; |Set new transform_key| Tiles\n\n  Tiles --&gt; |Select transform_key| Fuse\n\n  subgraph Input tile definition\n    subgraph Input-arrays\n        ArrayNP\n        ArrayDA\n        ArrayCP\n    end\n    Input-arrays --&gt; Tile_i\n    Scale --&gt; |Defines default transform_key| Tile_i\n    Dimensions --&gt; Tile_i\n    TileN\n  end\n\n  subgraph Registration\n  Graph-construction --&gt; Pairwise-registration\n  Pairwise-registration --&gt; Global-resolution\n  Global-resolution --&gt; Params\n  end\n\n  subgraph Fusion\n    direction LR\n\n  subgraph Fuse\n\n    direction TB\n\n    subgraph Fusion-methods[Fusion methods]\n      direction RL\n      Weighted_average\n      Max_fusion\n      Multi-view-deconvolution\n      Custom-fusion\n    end\n\n    subgraph Weighting-methods[Weighting methods]\n      direction RL\n      Blending_weights\n      Content_weights\n      Custom_weights\n    end\n\n    end\n\n    Fuse --&gt; Fused\n    Fused --&gt; Storage\n  end\n</code></pre>"},{"location":"code_example/","title":"Code example","text":"<p>These code snippets walk you through a small stitching workflow consisting of</p> <ol> <li>Preparing the input image data and metadata (tile positions, spacing, channels)</li> <li>Registering the tiles</li> <li>Stitching / fusing the tiles</li> </ol>"},{"location":"code_example/#1-prepare-data-for-stitching","title":"1) Prepare data for stitching","text":"<pre><code>import numpy as np\nfrom multiview_stitcher import msi_utils\nfrom multiview_stitcher import spatial_image_utils as si_utils\n\n# input data (can be any numpy compatible array: numpy, dask, cupy, etc.)\ntile_arrays = [np.random.randint(0, 100, (2, 10, 100, 100)) for _ in range(3)]\n\n# indicate the tile offsets and spacing\ntile_translations = [\n    {\"z\": 2.5, \"y\": -10, \"x\": 30},\n    {\"z\": 2.5, \"y\": 30, \"x\": 10},\n    {\"z\": 2.5, \"y\": 30, \"x\": 50},\n]\nspacing = {\"z\": 2, \"y\": 0.5, \"x\": 0.5}\n\nchannels = [\"DAPI\", \"GFP\"]\n\n# build input for stitching\nmsims = []\nfor tile_array, tile_translation in zip(tile_arrays, tile_translations):\n    sim = si_utils.get_sim_from_array(\n        tile_array,\n        dims=[\"c\", \"z\", \"y\", \"x\"],\n        scale=spacing,\n        translation=tile_translation,\n        transform_key=\"stage_metadata\",\n        c_coords=channels,\n    )\n    msims.append(msi_utils.get_msim_from_sim(sim, scale_factors=[]))\n\n# plot the tile configuration\n# from multiview_stitcher import vis_utils\n# fig, ax = vis_utils.plot_positions(msims, transform_key='stage_metadata', use_positional_colors=False)\n</code></pre>"},{"location":"code_example/#2-register-the-tiles","title":"2) Register the tiles","text":"<pre><code>from dask.diagnostics import ProgressBar\nfrom multiview_stitcher import registration\n\nwith ProgressBar():\n    params = registration.register(\n        msims,\n        reg_channel=\"DAPI\",  # channel to use for registration\n        transform_key=\"stage_metadata\",\n        new_transform_key=\"translation_registered\",\n    )\n\n# plot the tile configuration after registration\n# vis_utils.plot_positions(msims, transform_key='translation_registered', use_positional_colors=False)\n</code></pre>"},{"location":"code_example/#3-stitch-fuse-the-tiles","title":"3) Stitch / fuse the tiles","text":"<pre><code>from multiview_stitcher import fusion\n\nfused_sim = fusion.fuse(\n    [msi_utils.get_sim_from_msim(msim) for msim in msims],\n    transform_key=\"translation_registered\",\n)\n\n# get fused array as a dask array\nfused_sim.data\n\n# get fused array as a numpy array\nfused_sim.data.compute()\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are very welcome.</p>"},{"location":"data_formats/","title":"Data formats","text":"<p>Note</p> <p><code>multiview-stitcher</code> works with any numpy-like input arrays. Therefore, as long as the data can be read into a numpy array, it can be used with <code>multiview-stitcher</code>.</p> <p>For attaching metadata to arrays, multiview-stitcher works with SpatialImage objects (with additional transform matrices attached). They can be constructed from Numpy, Dask or CuPy arrays as such:</p> <pre><code>from multiview_stitcher import spatial_image_utils as si_utils\nsim = si_utils.get_sim_from_array(\n    tile_array,\n    dims=[\"c\", \"y\", \"x\"],\n    scale={'y': 0.5, 'x': 0.5},\n    translation={\"y\": 30, \"x\": 50},\n    transform_key=\"stage_metadata\",\n    c_coords=['DAPI', 'GFP'],\n)\n</code></pre> <p>A multiscale version of this object is represented by instances of MultiscaleSpatialImage, which can be created as such:</p> <pre><code>from multiview_stitcher import msi_utils\nmsim = msi_utils.get_msim_from_sim(sim, scale_factors=[2, 4])\n</code></pre> <p>The following code can be used to extract a given scale from a multiscale image:</p> <pre><code>sim = msi_utils.get_sim_from_msim(msim, scale=\"scale0\")\n</code></pre>"},{"location":"data_formats/#ome-zarr","title":"OME-Zarr","text":"<p>Note</p> <p>NGFF 0.4 (the latest OME-Zarr standard) currently only supports translation transformations. Therefore, affine transformations cannot yet be stored in OME-Zarr files.</p> <p>Some support for reading and writing OME-Zarrs is provided by multiscaleimage.MultiscaleImage.</p> <p>Further, <code>multiview_stitcher.ngff_utils</code> provides some convenience functions for reading and writing OME-Zarrs using <code>ngff-zarr</code>.</p>"},{"location":"data_formats/#further-file-formats","title":"Further file formats","text":"<p><code>bioio</code> is a very convenient library for reading a large variety of image files and it includes support for lazy loading. Here's example code of how to use <code>bioio</code> to load an image file into a tile compatible with <code>multiview-stitcher</code>:</p> <pre><code>from bioio import BioImage\nfrom multiview_stitcher import spatial_image_utils as si_utils\n\n# use bioio to load the image as a xarray.DataArray\nbioio_xr = BioImage(\"my_file.tiff\").get_xarray_dask_stack().squeeze()\n\n# ensure that dimension names are lowercase (expected by the get_sim_from_array function)\nbioio_xr = bioio_xr.rename(\n    {dim: dim.lower() for dim in bioio_xr.dims}\n    )\n\nsim = si_utils.get_sim_from_array(\n    bioio_xr.data,\n    dims=bioio_xr.dims,\n    scale=si_utils.get_spacing_from_sim(bioio_xr),      # dict of voxel sizes for each dim\n    translation=si_utils.get_origin_from_sim(bioio_xr), # dict of origin coordinates for each dim\n    c_coords=bioio_xr.coords[\"c\"].values,\n    transform_key=\"stage_metadata\",\n)\n</code></pre>"},{"location":"extension_api_fusion/","title":"Fusion","text":"<p>Custom functions can be passed to the <code>fusion.fuse</code> function. <code>multiview-stitcher</code> provides the custom fusion and weights functions with pre-transformed chunks corresponding to the <code>transform_key</code> passed to <code>fusion.fuse</code>. Only those views will be passed which contribute to the given output chunks. The elements of the input lists correspond to the different input views / tiles and conserve the order of the input views / tiles passed to <code>fusion.fuse</code>.</p> <pre><code>def fuse(\n    ...\n    transform_key: str = None,\n    fusion_func: Callable = weighted_average_fusion,\n    fusion_func_kwargs: dict = None,\n    weights_func: Callable = None, # by default no additional fusion weights are used\n    weights_func_kwargs: dict = None,\n    ...\n</code></pre>"},{"location":"extension_api_fusion/#custom-fusion-methods","title":"Custom fusion methods","text":"<p>Custom function for fusion of pre-transformed view chunks. This could implement e.g. a maximum intensity projection, a weighted average, or a multi-view deconvolution.</p> <pre><code>def custom_fusion_function(\n    transformed_views: List[Array-like], # list of pre-transformed view chunks\n    blending_weights: List[Array-like], # optional functional argument\n    fusion_weights: List[Array-like], # optional functional argument\n    **kwargs, # `fusion_func_kwargs` passed to `fusion.fuse`\n) -&gt; Array-like:\n\n    # Fusion code here\n\n    return fused_array\n</code></pre> <p>If the optional funtion arguments are not part of the function signature, the arguments will be ignored.</p> <p>Example implementation: <code>multiview_stitcher.fusion.weighted_average_fusion</code>.</p>"},{"location":"extension_api_fusion/#custom-weight-functions","title":"Custom weight functions","text":"<p>Custom function for calculating additional fusion weights passed to the fusion function as <code>fusion_weights</code>. This could implement e.g. a content-based weighting scheme. The function should return a list of weights, one for each view chunk.</p> <pre><code>def custom_weight_function(\n    transformed_views : List[Array-like],\n    blending_weights : List[Array-like],\n    **kwargs, # `weights_func_kwargs` passed to `fusion.fuse`\n) - &gt; List[Array-like]:\n\n    # Weight calculation code here\n\n    return weights\n</code></pre> <p>Example implementation: <code>multiview_stitcher.weights.content_based</code>.</p>"},{"location":"extension_api_global_param_resolution/","title":"Global parameter resolution","text":"<p>WIP: Custom function API to be added.</p>"},{"location":"extension_api_pairwise_registration/","title":"Pairwise registration","text":"<p>Custom functions for pairwise registration can be passed to the <code>registration.register</code> function:</p> <pre><code>def register(\n    ...\n    transform_key,\n    pairwise_reg_func: Callable = phase_correlation_registration,\n    pairwise_reg_func_kwargs: dict = None,\n    ...\n)\n</code></pre> <p>Custom registration functions can have one of the following two signatures. <code>registration.register</code> will automatically detect which signature is used and call the function accordingly, passing to the function - <code>pairwise_reg_func_kwargs</code> - the below detailed parameters to the functions.</p>"},{"location":"extension_api_pairwise_registration/#registration-in-pixel-space","title":"Registration in pixel space","text":"<p>This API is for adding registration functions that operate on pixel data without any knowledge of the physical space.</p> <p>Initial transformation</p> <p>The moving data array is pre-transformed to match the pixel coordinate space of the fixed data array, using the affine matrices of the <code>transform_key</code> passed to <code>registration.register</code>. Image data is passed as float dtype and NaN values mark pixels that map outside the image domains after transformation.</p> <p>Expected output transformation</p> <p>The affine matrix returned by a registration following this API transforms pixel indices of the fixed image to pixel indices of the moving image. <code>registration.register</code> will take care of converting this matrix to a transformation that transforms the physical space.</p> <pre><code>def pairwise_registration(\n    fixed_data: Array-like[np.float32],\n    moving_data: Array-like[np.float32],\n    **kwargs, # additional keyword arguments passed `pairwise_reg_func_kwargs`\n    ) -&gt; dict:\n\n    # Registration code here\n\n    return {\n        \"affine_matrix\": affine_matrix, # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": , # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre> <p>Note that in the case of pixel space registration, the input data is passed as <code>np.float32</code> arrays (as opposed to registration in physical coordinate space, during which the dtype remains unaltered). Invalid pixel values, i.e. those that map outside of the input data, are marked with NaNs.</p> <p>Example implementation: <code>multiview_stitcher.registration.phase_correlation_registration</code> (default pairwise registration function).</p>"},{"location":"extension_api_pairwise_registration/#registration-in-physical-coordinate-space","title":"Registration in physical coordinate space","text":"<p>This API is for adding registration functions that operate in physical coordinates.</p> <p>Transformations in physical space</p> <p>Both the pre-calculated <code>initial_affine</code> parameter passed to the pairwise registration function and the affine matrix returned by a registration following this API transform physical positions of the fixed image to physical positions of the moving image. Here, physical positions are calculated as <code>origin + spacing * pixel_index</code>.</p> <pre><code>def custom_pairwise_registration_function(\n    fixed_data: Array-like,\n    moving_data: Array-like,\n    *,\n    fixed_origin: dict[str, float], # e.g. {\"z\": 10.0, \"y\": 20.0, \"x\": 30.0}\n    moving_origin: dict[str, float],\n    fixed_spacing: dict[str, float], # e.g. {\"z\": 1.0, \"y\": 0.5, \"x\": 0.5}\n    moving_spacing: dict[str, float],\n    initial_affine: xr.DataArray, # see note below\n    **kwargs, # `pairwise_reg_func_kwargs` passed to `registration.register`\n) -&gt; dict:\n    # Registration code here\n\n    return {\n        \"affine_matrix\": affine_matrix, # homogenous matrix of shape (ndim + 1, ndim + 1), axis order (z, y, x)\n        \"quality\": 1.0, # float between 0 and 1 (if not available, set to 1.0)\n    }\n</code></pre> <p>For a description of the object that will be passed as an initial affine matrix, see here.</p>"},{"location":"extension_api_pairwise_registration/#fusion","title":"Fusion","text":"<p>Custom functions can be passed to the <code>fusion.fuse</code> function. <code>multiview-stitcher</code> provides the custom fusion and weights functions with pre-transformed chunks corresponding to the <code>transform_key</code> passed to <code>fusion.fuse</code>. Only those views will be passed which contribute to the given output chunks. The elements of the input lists correspond to the different input views / tiles and conserve the order of the input views / tiles passed to <code>fusion.fuse</code>.</p> <pre><code>def fuse(\n    ...\n    transform_key: str = None,\n    fusion_func: Callable = weighted_average_fusion,\n    fusion_func_kwargs: dict = None,\n    weights_func: Callable = None, # by default no additional fusion weights are used\n    weights_func_kwargs: dict = None,\n    ...\n</code></pre>"},{"location":"extension_api_pairwise_registration/#custom-fusion-methods","title":"Custom fusion methods","text":"<p>Custom function for fusion of pre-transformed view chunks. This could implement e.g. a maximum intensity projection, a weighted average, or a multi-view deconvolution.</p> <pre><code>def custom_fusion_function(\n    transformed_views: List[Array-like], # list of pre-transformed view chunks\n    blending_weights: List[Array-like], # optional functional argument\n    fusion_weights: List[Array-like], # optional functional argument\n    **kwargs, # `fusion_func_kwargs` passed to `fusion.fuse`\n) -&gt; Array-like:\n\n    # Fusion code here\n\n    return fused_array\n</code></pre> <p>If the optional funtion arguments are not part of the function signature, the arguments will be ignored.</p> <p>Example implementation: <code>multiview_stitcher.fusion.weighted_average_fusion</code>.</p>"},{"location":"extension_api_pairwise_registration/#custom-weight-functions","title":"Custom weight functions","text":"<p>Custom function for calculating additional fusion weights passed to the fusion function as <code>fusion_weights</code>. This could implement e.g. a content-based weighting scheme. The function should return a list of weights, one for each view chunk.</p> <pre><code>def custom_weight_function(\n    transformed_views : List[Array-like],\n    blending_weights : List[Array-like],\n    **kwargs, # `weights_func_kwargs` passed to `fusion.fuse`\n) - &gt; List[Array-like]:\n\n    # Weight calculation code here\n\n    return weights\n</code></pre> <p>Example implementation: <code>multiview_stitcher.weights.content_based</code>.</p>"},{"location":"extension_api_pairwise_registration/#global-parameter-resolution","title":"Global parameter resolution","text":"<p>Custom function API to be added.</p>"},{"location":"features/","title":"Features","text":"<p>Below is a list of features which are either already implemented or are on the roadmap.</p>"},{"location":"features/#dimensionality","title":"Dimensionality","text":"<ul> <li> 2D</li> <li> 3D</li> </ul>"},{"location":"features/#registration","title":"Registration","text":""},{"location":"features/#pairwise-registration-methods","title":"Pairwise registration methods","text":"<ul> <li> Phase correlation</li> <li> ANTsPy</li> <li> Elastix (<code>itk-elastix</code>)</li> <li> Bead alignment</li> <li> Phase correlation for rotation + translation</li> </ul>"},{"location":"features/#global-paramater-resolution","title":"Global paramater resolution","text":"<ul> <li> Graph construction</li> <li> Automatic determination of reference view</li> <li> Parameter concatenation along graph connectivity paths</li> <li> Global optimization of registration parameters from (potentially overdetermined) pairwise transforms</li> </ul>"},{"location":"features/#transformations","title":"Transformations","text":"<ul> <li> Chunked <code>dask_image.ndinterp.affine_transform</code></li> <li> Cupy-based transform</li> <li> Chaining transformations instead of working with static coordinate systems</li> </ul>"},{"location":"features/#fusion","title":"Fusion","text":""},{"location":"features/#general","title":"General","text":"<ul> <li> Modular API to plug in different fusion and weight functions</li> <li> Support for fusion label maps</li> <li> Cupy-based fusion</li> </ul>"},{"location":"features/#supported-fusion-methods","title":"Supported fusion methods:","text":"<ul> <li> Weighted average</li> <li> Maximum intensity projection</li> <li> Multi-view deconvolution</li> </ul>"},{"location":"features/#supported-weights","title":"Supported weights:","text":"<ul> <li> Linear blending</li> <li> Content-based</li> </ul>"},{"location":"features/#supported-data-formats","title":"Supported data formats","text":"<ul> <li> OME-Zarr</li> <li> Zarr based intermediate file format for reading and writing, compatible with parallel dask workflows: multiscale-spatial-image</li> <li>CZI input</li> <li> Multi-positioning</li> <li> Light-sheet</li> <li> TIF input</li> <li> TIF writing</li> </ul>"},{"location":"features/#visualization","title":"Visualization","text":""},{"location":"features/#napari","title":"Napari","text":"<p>See napari-stitcher.</p> <ul> <li> 2D slice view: multiscale rendering</li> <li>3D rendered view:</li> <li> Lowest scale</li> <li> Chunked rendering</li> <li> Colormaps optimized for highlighting differences between overlapping views</li> </ul>"},{"location":"features/#supported-usage-modes","title":"Supported usage modes","text":"<ul> <li> As a library to build custom reconstruction workflows</li> <li> Napari plugin</li> <li> Convenience function for processing on HPC</li> </ul>"},{"location":"implementation_details/","title":"Implementation details","text":""},{"location":"implementation_details/#image-data-structures","title":"(Image) data structures","text":""},{"location":"implementation_details/#affine-transformations","title":"Affine transformations","text":"<p>Affine transformations associated to an image / view are represented as <code>xarray.DataArray</code>s with dimensions (t, x_in, x_out), typically of shape (N_tps, ndim+1, ndim+1). There's one transform per timepoint.</p>"},{"location":"implementation_details/#in-memory-representation","title":"In memory representation","text":""},{"location":"implementation_details/#spatial-image","title":"spatial-image","text":"<p>Subclasses <code>xarray.DataArray</code>, i.e. broadly speaking these are numpy/dask arrays with axis labels, coordinates and attributes. spatial-image is dask compatible for lazy loading and parallel processing.</p>"},{"location":"implementation_details/#multiscale-spatial-image","title":"multiscale-spatial-image","text":"<ul> <li><code>xarray.datatree</code> containing one <code>xarray.Dataset</code> per (hierarchical) spatial scale</li> <li>these are collections of <code>xarray.DataArray</code> which are called \"data variables\" and share coordinates.</li> <li>each scale contains a <code>spatial-image</code>s as a data variable named 'image'</li> <li>compatible with NGFF (github.com/ome/ngff)</li> <li>can be (de-)serialized to zarr</li> <li>also used by github.com/scverse/spatialdata</li> </ul>"},{"location":"implementation_details/#coordinate-systems","title":"Coordinate systems","text":"<p>spatial-image, multiscale-spatial-image, as well as NGFF (as of 0.4.1), do not yet support:</p> <ul> <li>affine transformations</li> <li>different transformations for different timepoints.</li> </ul> <p>However, affine transformations are important for positioning views relatively to each other. Therefore, <code>spatial-image</code> and <code>multiscale-spatial-image</code> are used with modifications. Specifically, affine transformation parameters which transform the image into a coordinate system of a given name are attached to both: - <code>spatial-image</code>: as key(name)/value pairs under a 'transform' attribute - <code>multiscale-spatial-image</code>: to each scale as data variables, sharing the 't' coordinate with the associated image data variable. This is compatible with reading and writing <code>multiscale_spatial_image.to_zarr()</code> and <code>datatree.open_zarr()</code>.</p> <p>In the code, coordinate systems are referred to as transform_key (TODO: find better name, e.g. coordinate_system).</p>"},{"location":"implementation_details/#registration","title":"Registration","text":""},{"location":"implementation_details/#overlap-graph","title":"Overlap graph","text":"<p>An overlap graph is computed from the input images (represented as a directed <code>networkx.DiGraph</code>) in which the - nodes represent the views and - edges represent geometrical overlap.</p> <p>This graph can be used to conveniently color views for visualization (overlapping views should have different colors, but the total number of colors used shouldn't be too large, i.e. exceed 2-4).</p>"},{"location":"implementation_details/#reference-view","title":"Reference view","text":"<p>A suitable reference view can be obtained from the overlap graph by e.g. choosing the view with maximal overlap to other views.</p>"},{"location":"implementation_details/#registration-graph","title":"Registration graph","text":"<p>A registration graph or list of registration pairs (TODO: clarify whether this should be a graph or a list of pairs) is obtained from the overlap graph by e.g. finding shortest overlap-weighted paths between the reference view and all other views.</p>"},{"location":"implementation_details/#fusion","title":"Fusion","text":""},{"location":"implementation_details/#fusion-framework","title":"Fusion framework","text":"<p><code>multiview_stitcher.fusion.fuse(..., fusion_func=fusion.weighted_average_fusion)</code> can be used in combination with fusion functions available in <code>multiview_stitcher.fusion</code> or custom functions that accept the following keyword arguments:</p> <pre><code>transformed_views : list of ndarrays\n    transformed input views\nblending_weights : list of ndarrays, optional\n    blending weights for each view\nfusion_weights : list of ndarrays, optional\n    additional view weights for fusion, e.g. contrast weighted scores.\nparams : list of xarrays, optional\n</code></pre> <p>Further fusion weights can be obtained by the weight calculation function specified in <code>fuse(..., weights_func=None, weights_func_kwargs=None)</code>. An example for such a function is <code>weights.content_based</code>, but also custom functions can be passed which accept the same (optional) input arguments as the fusion functions.</p>"},{"location":"implementation_details/#content-based-fusion","title":"Content based fusion","text":"<p>To improve multi-view fusion in the context of strongly scattering samples, content-based fusion turns out to be helpful. This fusion method is available in <code>multiview-stitcher</code> by using <code>multiview_stitcher.fusion.fuse(..., fusion_func=fusion.weighted_average_fusion, weights_method=weights.content_based)</code>.</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install <code>multiview-stitcher</code> via pip:</p> <pre><code>pip install multiview-stitcher\n</code></pre> <p>To install latest development version:</p> <pre><code>pip install git+https://github.com/multiview-stitcher/multiview-stitcher.git\n</code></pre>"},{"location":"napari_stitcher/","title":"Napari plugin","text":"<p><code>multiview-stitcher</code> has an associated napari plugin, napari-stitcher.</p> <p>A space we're watching closely is the advancement of napari towards multi-scale 3D rendering and improved 3D interactivity.</p> <p></p>"},{"location":"neuroglancer/","title":"Neuroglancer (web-based)","text":"<p>Neuroglancer</p> <p>Neuroglancer is a powerful web-based viewer for large image datasets. (Multiple) images can be visualized by simply including URLs to the image data in a neuroglancer link (example), no installation required.</p>"},{"location":"neuroglancer/#neuroglancer-support-in-multiview-stitcher","title":"Neuroglancer support in <code>multiview-stitcher</code>","text":"<p>Neuroglancer supports visualizing multiple OME-Zarr files and reads their corresponding scale and translation metadata. Additionally, neuroglancer supports attaching affine transformations to each image. This allows visualizing spatial images (<code>sims</code>) together with the transforms attached in a given <code>transform_key</code>.</p> <p>multiview-stitcher provides the utility function <code>vis_utils.view_neuroglancer</code> that: - creates a neuroglancer link given a list of OME-Zarr paths - serves the OME-Zarrs over http in case they represent local files - optionally includes transforms from a given <code>transform_key</code> in the <code>sims</code> in the neuroglancer link - opens the neuroglancer link in the browser</p>"},{"location":"neuroglancer/#examples","title":"Examples","text":""},{"location":"neuroglancer/#viewing-a-list-of-ome-zarrs","title":"Viewing a list of OME-Zarrs","text":"<pre><code>from multiview_stitcher import vis_utils\n\nome_zarr_paths = [\n    \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0101A/13457537.zarr\",\n    \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0101A/13457227.zarr\"\n]\n\nvis_utils.view_neuroglancer(\n    ome_zarr_paths=ome_zarr_paths,\n)\n</code></pre>"},{"location":"neuroglancer/#viewing-a-list-of-spatial-images-with-attached-transforms","title":"Viewing a list of spatial images with attached transforms","text":"<pre><code>from multiview_stitcher import ngff_utils, vis_utils\n\n# if not already existing, write sims to OME-Zarrs\n# (or \"persist\" sims to OME-Zarr)\n\nome_zarr_paths = [f\"/tmp/ome_zarr_{isim}.zarr\"\n    for isim in range(len(sims))]\n\nsims = [ngff_utils.write_sim_to_ome_zarr(sim, path)\n    for zip(path, ome_zarr_paths)]\n\n# view transform_key \"registered\" using neuroglancer\nvis_utils.view_neuroglancer(\n    ome_zarr_paths=ome_zarr_paths,\n    sims=sims,\n    transform_key=\"registered\",\n)\n</code></pre> <p>Limitation: Both OME-Zarr and neuroglancer currently don't allow assigning different transforms to different time points or channels.</p>"},{"location":"neuroglancer/#more-examples","title":"More examples","text":"<p>See the usage of <code>view_neuroglancer</code> in the NGFF notebooks.</p>"},{"location":"notebooks/","title":"Notebooks","text":"<p>Example notebooks can be found here: multiview-stitcher/notebooks.</p>"},{"location":"objects/","title":"Objects","text":""},{"location":"objects/#image","title":"Image","text":"<p>Modified instances of <code>multiscaleimage.MultiscaleImage</code>.</p> <p>Modification: Each scale has at least one named affine transform parameter attached to it as further data variables next to <code>scale&lt;scale&gt;/image</code>.</p> <p>While instances of <code>multiscale-spatial-image</code> can be serialized to and from NGFF, modified instances of <code>multiscaleimage.MultiscaleImage</code> as used by <code>multiview-stitcher</code> cannot (yet) be serialized to and from NGFF (see here), as the support for affine transforms is missing.</p> <p>Example string representation:</p> <p><code>print(msims[0])</code> <pre><code>DataTree('None', parent=None)\n\u2502   Dimensions:  ()\n\u2502   Data variables:\n\u2502       *empty*\n\u2502   Attributes:\n\u2502       multiscaleSpatialImageVersion:  1\n\u2502       multiscales:                    [{'@type': 'ngff:Image', 'axes': [{'name'...\n\u251c\u2500\u2500 DataTree('scale0')\n\u2502       Dimensions:          (t: 1, x_in: 4, x_out: 4, c: 1, z: 179, y: 1040, x: 1392)\n\u2502       Coordinates:\n\u2502         * c                (c) int64 0\n\u2502         * t                (t) int64 0\n\u2502         * x                (x) float64 0.0 0.645 1.29 1.935 ... 895.9 896.6 897.2\n\u2502         * y                (y) float64 0.0 0.645 1.29 1.935 ... 668.9 669.5 670.2\n\u2502         * z                (z) float64 0.0 2.58 5.16 7.74 ... 451.5 454.1 456.7 459.2\n\u2502       Dimensions without coordinates: x_in, x_out\n\u2502       Data variables:\n\u2502           affine_metadata  (t, x_in, x_out) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n\u2502           image            (t, c, z, y, x) uint16 dask.array&lt;chunksize=(1, 1, 179, 256, 256), meta=np.ndarray&gt;\n\u251c\u2500\u2500 DataTree('scale1')\n\u2502       Dimensions:          (t: 1, x_in: 4, x_out: 4, c: 1, z: 179, y: 520, x: 696)\n\u2502       Coordinates:\n\u2502         * c                (c) int64 0\n\u2502         * t                (t) int64 0\n\u2502         * x                (x) float64 0.3225 1.613 2.902 4.193 ... 894.3 895.6 896.9\n\u2502         * y                (y) float64 0.3225 1.613 2.902 4.193 ... 667.3 668.5 669.8\n\u2502         * z                (z) float64 0.0 2.58 5.16 7.74 ... 451.5 454.1 456.7 459.2\n\u2502       Dimensions without coordinates: x_in, x_out\n\u2502       Data variables:\n\u2502           affine_metadata  (t, x_in, x_out) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n\u2502           image            (t, c, z, y, x) uint16 dask.array&lt;chunksize=(1, 1, 179, 256, 256), meta=np.ndarray&gt;\n\u2514\u2500\u2500 DataTree('scale2')\n        Dimensions:          (t: 1, x_in: 4, x_out: 4, c: 1, z: 179, y: 260, x: 348)\n        Coordinates:\n          * c                (c) int64 0\n          * t                (t) int64 0\n          * x                (x) float64 0.9675 3.548 6.128 8.707 ... 891.1 893.6 896.2\n          * y                (y) float64 0.9675 3.548 6.128 8.707 ... 664.0 666.6 669.2\n          * z                (z) float64 0.0 2.58 5.16 7.74 ... 451.5 454.1 456.7 459.2\n        Dimensions without coordinates: x_in, x_out\n        Data variables:\n            affine_metadata  (t, x_in, x_out) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n            image            (t, c, z, y, x) uint16 dask.array&lt;chunksize=(1, 1, 179, 256, 256), meta=np.ndarray&gt;\n</code></pre></p>"},{"location":"objects/#transformation-parameters","title":"Transformation parameters","text":""},{"location":"objects/#affine-transformation-parameters","title":"Affine transformation parameters","text":"<p><code>xarray.DataArray</code> containing parameters in the form of - a homogeneous transform matrix - of dimensionality (ndim+1, ndim+1) - datatype <code>float</code></p> <p>with axis labels - 't' - 'x_in' - 'x_out'</p> <p>Example string representation:</p> <p><code>print(msims[0]['scale0/affine_manual'])</code> <pre><code>&lt;xarray.DataArray 'affine_manual' (t: 1, x_in: 4, x_out: 4)&gt;\narray([[[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]]])\nCoordinates:\n  * t        (t) int64 0\nDimensions without coordinates: x_in, x_out\n</code></pre></p>"},{"location":"related_projects/","title":"Related projects","text":""},{"location":"related_projects/#related-software","title":"Related software","text":"<ul> <li>BigStitcher</li> <li>ashlar</li> <li>TeraStitcher</li> <li>m2stitch</li> </ul>"},{"location":"related_projects/#other-cool-spaces","title":"Other cool spaces","text":"<ul> <li>ndpyramid</li> <li>affinder</li> <li>bigstream</li> <li>SpatialData</li> </ul>"},{"location":"stitching_in_the_browser/","title":"Stitching in the browser","text":"<p><code>multiview-stitcher</code> can run without installation in your browser.</p>"},{"location":"stitching_in_the_browser/#try-it-out","title":"Try it out","text":"<ul> <li>open JupyterLite in a private browser window</li> <li>upload this notebook into the jupyter lab window: notebooks/stitching_in_the_browser.ipynb</li> <li>upload files to stitch into a 'data' folder in the jupyter lab window</li> <li>follow the notebook</li> </ul>"},{"location":"stitching_in_the_browser/#limitations","title":"Limitations","text":"<ul> <li>stitching will run with a single thread</li> <li>while the code runs locally, your local file system is not directly accessible from within the browser environment</li> </ul>"},{"location":"stitching_in_the_browser/#this-cool-functionality-is-possible-thanks-to","title":"This cool functionality is possible thanks to","text":"<ul> <li>JupyterLite, a JupyterLab distribution that runs in the browser</li> <li>pyodide, a Python runtime for the browser</li> </ul>"},{"location":"troubleshoot_registration_OOM/","title":"MemoryError","text":"<p>Here are some things to do if you encounter a <code>MemoryError</code> during registration.</p>"},{"location":"troubleshoot_registration_OOM/#use-precomputed-resolution-levels","title":"Use precomputed resolution levels","text":"<p>If your data has precomputed multiscale pyramids (e.g., from OME-Zarr), you can use the <code>reg_res_level</code> parameter to directly use a lower resolution level for registration, which is more memory-efficient than binning at runtime:</p> <pre><code>register(\n  ...,\n  reg_res_level=1,  # use scale1 (first downsampled level)\n  )\n</code></pre> <p>Alternatively, you can let the registration automatically select the best precomputed resolution level by specifying <code>registration_binning</code>:</p> <pre><code>register(\n  ...,\n  registration_binning={'z': 2, 'y': 4, 'x': 4},  # automatically uses best matching resolution level\n  )\n</code></pre>"},{"location":"troubleshoot_registration_OOM/#increase-registration-binning","title":"Increase registration binning","text":"<ol> <li>Increase the registration binning e.g. to sth like <code>{'z': 2, 'y': 4, 'x': 4}</code>. Aim for factors that lead to isotropic and downsampled spacing for registration. Probably even a factor of ~4 can produce accurate subpixel registration parameters (with respect to binning 1).</li> </ol> <pre><code>register(\n  ...,\n  registration_binning={'z': 2, 'y': 4, 'x': 4},\n  )\n</code></pre>"},{"location":"troubleshoot_registration_OOM/#decrease-computation-parallelism","title":"Decrease computation parallelism","text":"<p>By default, the registration of all the pairs to be registered is run in parallel. Therefore reducing the parallelism can help. This can be done by setting <code>register(..., n_parallel_pairwise_regs=4)</code>, which in this example will limit the pairwise registrations that are run in parallel to four.</p> <p>Estimating memory requirements</p> <p>The memory required for each pairwise registration can be estimated by considering that the overlapping image regions need to be loaded in memory (taking into consideration the binning factors), multiplied by a factor of approx. 2-4 for the registration process (depending on the registration function used).</p>"},{"location":"troubleshoot_registration_accuracy/","title":"Accuracy","text":"<p>Here are some things to do if you encounter issues with the accuracy of the registration.</p>"},{"location":"troubleshoot_registration_accuracy/#determining-which-pairs-of-tilesviews-are-registered","title":"Determining which pairs of tiles/views are registered","text":"<p>Do you have a regular grid of input tiles? In that case it's recommended to use <code>register(..., pre_registration_pruning=\"keep_axis_aligned\")</code> available in multiview-stitcher version&gt;=0.18, which disregards diagonal overlaps during pairwise registration.</p> <pre><code>register(\n  ...,\n  pre_registration_pruning=\"keep_axis_aligned\", # None, 'keep_axis_aligned', 'shortest_paths_overlap_weighted'\n  )\n</code></pre>"},{"location":"troubleshoot_registration_accuracy/#registration-binning","title":"Registration binning","text":"<ol> <li>Decrease the registration binning, with {'z': 1, 'y': 1, 'x': 1} performing the registration on the highest possible resolution.</li> </ol> <p>Binning</p> <p>Keep in mind that typically not much is gained from binning factors lower than 2.</p>"},{"location":"troubleshoot_registration_accuracy/#choosing-the-right-method-for-pairwise-registration","title":"Choosing the right method for pairwise registration","text":""},{"location":"troubleshoot_registration_accuracy/#type-of-transform","title":"Type of transform","text":"<p>Consider which transform suits your data best: 1. Translation 1. Rigid 1. Similarity 1. Affine</p>"},{"location":"troubleshoot_registration_accuracy/#available-registration-implementations","title":"Available registration implementations:","text":"<ol> <li>Phase correlation (translation)</li> <li>AntsP (translation, rigid, similarity, affine)</li> <li>ITK-elastix (translation, rigid, similarity, affine) (WIP)</li> <li>Custom registration functions (up to affine)</li> </ol>"},{"location":"troubleshoot_registration_accuracy/#change-the-parameters-used-during-global-parameter-resolution","title":"Change the parameters used during global parameter resolution","text":""},{"location":"troubleshoot_registration_accuracy/#type-of-transform_1","title":"Type of transform","text":"<p>Following pairwise registration, determining the global transformation parameters can also be done assuming different types of transforms:   1. Translation   1. Rigid   1. Similarity   1. Affine</p> <p>Example: <pre><code>registration.register(\n  ...\n  groupwise_resolution_kwargs={\n      'transform': 'translation', # 'rigid', 'similarity', 'affine'\n  }\n)\n</code></pre></p>"},{"location":"troubleshoot_registration_accuracy/#alternative-simple-global-parameter-resolution","title":"Alternative: simple global parameter resolution","text":"<pre><code>registration.register(\n  ...\n  groupwise_resolution_method='shortest_paths',\n  groupwise_resolution_kwargs={\n      # no options available\n  }\n)\n</code></pre>"},{"location":"api/fusion/","title":"Fusion","text":""},{"location":"api/fusion/#multiview_stitcher.fusion.fuse","title":"<code>fuse(sims, transform_key=None, fusion_func=weighted_average_fusion, fusion_method_kwargs=None, weights_func=None, weights_func_kwargs=None, output_spacing=None, output_stack_mode='union', output_origin=None, output_shape=None, output_stack_properties=None, output_chunksize=None, overlap_in_pixels=None, interpolation_order=1, blending_widths=None)</code>","text":"<p>Fuse input views.</p> <p>This function fuses all (Z)YX views (\"fields\") contained in the input list of images, which can additionally contain C and T dimensions.</p>"},{"location":"api/fusion/#multiview_stitcher.fusion.fuse--parameters","title":"Parameters","text":"<p>sims : list of SpatialImage     Input views. transform_key : str, optional     Which (extrinsic coordinate system) to use as transformation parameters.     By default None (intrinsic coordinate system). fusion_func : Callable, optional     Fusion function to be applied. This function receives the following     inputs (as arrays if applicable): transformed_views, blending_weights, fusion_weights, params.     By default weighted_average_fusion fusion_method_kwargs : dict, optional weights_func : Callable, optional     Function to calculate fusion weights. This function receives the     following inputs: transformed_views (as spatial images), params.     It returns (non-normalized) fusion weights for each view.     By default None. weights_func_kwargs : dict, optional output_spacing : dict, optional     Spacing of the fused image for each spatial dimension, by default None output_stack_mode : str, optional     Mode to determine output stack properties. Can be one of     \"union\", \"intersection\", \"sample\". By default \"union\" output_origin : dict, optional     Origin of the fused image for each spatial dimension, by default None output_shape : dict, optional     Shape of the fused image for each spatial dimension, by default None output_stack_properties : dict, optional     Dictionary describing the output stack with keys     'spacing', 'origin', 'shape'. Other output_* are ignored     if this argument is present. output_chunksize : int or dict, optional     Chunksize of the dask data array of the fused image. If the first tile is a chunked dask array,     its chunksize is used as the default. If the first tile is not a chunked dask array,     the default chunksize defined in spatial_image_utils.py is used.</p>"},{"location":"api/fusion/#multiview_stitcher.fusion.fuse--returns","title":"Returns","text":"<p>SpatialImage     Fused image.</p> Source code in <code>src/multiview_stitcher/fusion.py</code> <pre><code>def fuse(\n    sims: list,\n    transform_key: str = None,\n    fusion_func: Callable = weighted_average_fusion,\n    fusion_method_kwargs: dict = None,\n    weights_func: Callable = None,\n    weights_func_kwargs: dict = None,\n    output_spacing: dict[str, float] = None,\n    output_stack_mode: str = \"union\",\n    output_origin: dict[str, float] = None,\n    output_shape: dict[str, int] = None,\n    output_stack_properties: BoundingBox = None,\n    output_chunksize: Union[int, dict[str, int]] = None,\n    overlap_in_pixels: int = None,\n    interpolation_order: int = 1,\n    blending_widths: dict[str, float] = None,\n):\n    \"\"\"\n\n    Fuse input views.\n\n    This function fuses all (Z)YX views (\"fields\") contained in the\n    input list of images, which can additionally contain C and T dimensions.\n\n    Parameters\n    ----------\n    sims : list of SpatialImage\n        Input views.\n    transform_key : str, optional\n        Which (extrinsic coordinate system) to use as transformation parameters.\n        By default None (intrinsic coordinate system).\n    fusion_func : Callable, optional\n        Fusion function to be applied. This function receives the following\n        inputs (as arrays if applicable): transformed_views, blending_weights, fusion_weights, params.\n        By default weighted_average_fusion\n    fusion_method_kwargs : dict, optional\n    weights_func : Callable, optional\n        Function to calculate fusion weights. This function receives the\n        following inputs: transformed_views (as spatial images), params.\n        It returns (non-normalized) fusion weights for each view.\n        By default None.\n    weights_func_kwargs : dict, optional\n    output_spacing : dict, optional\n        Spacing of the fused image for each spatial dimension, by default None\n    output_stack_mode : str, optional\n        Mode to determine output stack properties. Can be one of\n        \"union\", \"intersection\", \"sample\". By default \"union\"\n    output_origin : dict, optional\n        Origin of the fused image for each spatial dimension, by default None\n    output_shape : dict, optional\n        Shape of the fused image for each spatial dimension, by default None\n    output_stack_properties : dict, optional\n        Dictionary describing the output stack with keys\n        'spacing', 'origin', 'shape'. Other output_* are ignored\n        if this argument is present.\n    output_chunksize : int or dict, optional\n        Chunksize of the dask data array of the fused image. If the first tile is a chunked dask array,\n        its chunksize is used as the default. If the first tile is not a chunked dask array,\n        the default chunksize defined in spatial_image_utils.py is used.\n\n    Returns\n    -------\n    SpatialImage\n        Fused image.\n    \"\"\"\n\n    output_chunksize = process_output_chunksize(sims, output_chunksize)\n\n    output_stack_properties = process_output_stack_properties(\n        sims=sims,\n        output_spacing=output_spacing,\n        output_origin=output_origin,\n        output_shape=output_shape,\n        output_stack_properties=output_stack_properties,\n        output_stack_mode=output_stack_mode,\n        transform_key=transform_key,\n    )\n\n    sdims = si_utils.get_spatial_dims_from_sim(sims[0])\n    nsdims = si_utils.get_nonspatial_dims_from_sim(sims[0])\n\n    params = [\n        si_utils.get_affine_from_sim(sim, transform_key=transform_key)\n        for sim in sims\n    ]\n\n    # determine overlap from weights method\n    # (soon: fusion methods will also require overlap)\n    overlap_in_pixels = 0\n    if weights_func is not None:\n        overlap_in_pixels = np.max(\n            [\n                overlap_in_pixels,\n                weights.calculate_required_overlap(\n                    weights_func, weights_func_kwargs\n                ),\n            ]\n        )\n\n    # calculate output chunk bounding boxes\n    output_chunk_bbs, block_indices = mv_graph.get_chunk_bbs(\n        output_stack_properties, output_chunksize\n    )\n\n    # add overlap to output chunk bounding boxes\n    output_chunk_bbs_with_overlap = [\n        output_chunk_bb\n        | {\n            \"origin\": {\n                dim: output_chunk_bb[\"origin\"][dim]\n                - overlap_in_pixels * output_stack_properties[\"spacing\"][dim]\n                for dim in sdims\n            }\n        }\n        | {\n            \"shape\": {\n                dim: output_chunk_bb[\"shape\"][dim] + 2 * overlap_in_pixels\n                for dim in sdims\n            }\n        }\n        for output_chunk_bb in output_chunk_bbs\n    ]\n\n    views_bb = [si_utils.get_stack_properties_from_sim(sim) for sim in sims]\n\n    merges = []\n    for ns_coords in itertools.product(\n        *tuple([sims[0].coords[nsdim] for nsdim in nsdims])\n    ):\n        sim_coord_dict = {\n            ndsim: ns_coords[i] for i, ndsim in enumerate(nsdims)\n        }\n        params_coord_dict = {\n            ndsim: ns_coords[i]\n            for i, ndsim in enumerate(nsdims)\n            if ndsim in params[0].dims\n        }\n\n        # ssims = [sim.sel(sim_coord_dict) for sim in sims]\n        sparams = [param.sel(params_coord_dict) for param in params]\n\n        # should this be done within the loop over output chunks?\n        fix_dims = []\n        for dim in sdims:\n            other_dims = [odim for odim in sdims if odim != dim]\n            if (\n                any((param.sel(x_in=dim, x_out=dim) - 1) for param in sparams)\n                or any(\n                    any(param.sel(x_in=dim, x_out=other_dims))\n                    for param in sparams\n                )\n                or any(\n                    any(param.sel(x_in=other_dims, x_out=dim))\n                    for param in sparams\n                )\n                or any(\n                    output_stack_properties[\"spacing\"][dim]\n                    - views_bb[iview][\"spacing\"][dim]\n                    for iview in range(len(sims))\n                )\n                or any(\n                    float(\n                        output_stack_properties[\"origin\"][dim]\n                        - param.sel(x_in=dim, x_out=\"1\")\n                    )\n                    % output_stack_properties[\"spacing\"][dim]\n                    for param in sparams\n                )\n            ):\n                continue\n            fix_dims.append(dim)\n\n        fused_output_chunks = np.empty(\n            np.max(block_indices, 0) + 1, dtype=object\n        )\n\n        for output_chunk_bb, output_chunk_bb_with_overlap, block_index in zip(\n            output_chunk_bbs, output_chunk_bbs_with_overlap, block_indices\n        ):\n            # calculate relevant slices for each output chunk\n            # this is specific to each non spatial coordinate\n            views_overlap_bb = [\n                mv_graph.get_overlap_for_bbs(\n                    target_bb=output_chunk_bb_with_overlap,\n                    query_bbs=[view_bb],\n                    param=sparams[iview],\n                    additional_extent_in_pixels={\n                        dim: 0 if dim in fix_dims else int(interpolation_order)\n                        for dim in sdims\n                    },\n                )[0]\n                for iview, view_bb in enumerate(views_bb)\n            ]\n\n            # append to output\n            relevant_view_indices = np.where(\n                [\n                    view_overlap_bb is not None\n                    for view_overlap_bb in views_overlap_bb\n                ]\n            )[0]\n\n            if not len(relevant_view_indices):\n                fused_output_chunks[tuple(block_index)] = da.zeros(\n                    tuple([output_chunk_bb[\"shape\"][dim] for dim in sdims]),\n                    dtype=sims[0].dtype,\n                )\n                continue\n\n            tol = 1e-6\n            sims_slices = [\n                sims[iview].sel(\n                    sim_coord_dict\n                    | {\n                        dim: slice(\n                            views_overlap_bb[iview][\"origin\"][dim] - tol,\n                            views_overlap_bb[iview][\"origin\"][dim]\n                            + (views_overlap_bb[iview][\"shape\"][dim] - 1)\n                            * views_overlap_bb[iview][\"spacing\"][dim]\n                            + tol,\n                        )\n                        for dim in sdims\n                    },\n                    drop=True,\n                )\n                for iview in relevant_view_indices\n            ]\n\n            # determine whether to fuse plany by plane\n            #  to avoid weighting edge artifacts\n            # fuse planewise if:\n            # - z dimension is present\n            # - params don't affect z dimension\n            # - shape in z dimension is 1 (i.e. only one plane)\n            # (the last criterium above could be dropped if we find a way\n            # (to propagate metadata through xr.apply_ufunc)\n\n            if (\n                \"z\" in fix_dims\n                and output_chunk_bb_with_overlap[\"shape\"][\"z\"] == 1\n            ):\n                fuse_planewise = True\n\n                sims_slices = [sim.isel(z=0) for sim in sims_slices]\n                tmp_params = [\n                    sparams[iview].sel(\n                        x_in=[\"y\", \"x\", \"1\"],\n                        x_out=[\"y\", \"x\", \"1\"],\n                    )\n                    for iview in relevant_view_indices\n                ]\n\n                output_chunk_bb_with_overlap = mv_graph.project_bb_along_dim(\n                    output_chunk_bb_with_overlap, dim=\"z\"\n                )\n\n                full_view_bbs = [\n                    mv_graph.project_bb_along_dim(views_bb[iview], dim=\"z\")\n                    for iview in relevant_view_indices\n                ]\n\n            else:\n                fuse_planewise = False\n                tmp_params = [\n                    sparams[iview] for iview in relevant_view_indices\n                ]\n                full_view_bbs = [\n                    views_bb[iview] for iview in relevant_view_indices\n                ]\n\n            fused_output_chunk = delayed(\n                lambda append_leading_axis, **kwargs: fuse_np(**kwargs)[\n                    np.newaxis\n                ]\n                if append_leading_axis\n                else fuse_np(**kwargs),\n            )(\n                append_leading_axis=fuse_planewise,\n                sims=sims_slices,\n                params=tmp_params,\n                output_properties=output_chunk_bb_with_overlap,\n                fusion_func=fusion_func,\n                fusion_method_kwargs=fusion_method_kwargs,\n                weights_func=weights_func,\n                weights_func_kwargs=weights_func_kwargs,\n                trim_overlap_in_pixels=overlap_in_pixels,\n                interpolation_order=1,\n                full_view_bbs=full_view_bbs,\n                blending_widths=blending_widths,\n            )\n\n            fused_output_chunk = da.from_delayed(\n                fused_output_chunk,\n                shape=tuple([output_chunk_bb[\"shape\"][dim] for dim in sdims]),\n                dtype=sims[0].dtype,\n            )\n\n            fused_output_chunks[tuple(block_index)] = fused_output_chunk\n\n        fused = da.block(fused_output_chunks.tolist())\n\n        merge = si.to_spatial_image(\n            fused,\n            dims=sdims,\n            scale=output_stack_properties[\"spacing\"],\n            translation=output_stack_properties[\"origin\"],\n        )\n\n        merge = merge.expand_dims(nsdims)\n        merge = merge.assign_coords(\n            {ns_coord.name: [ns_coord.values] for ns_coord in ns_coords}\n        )\n        merges.append(merge)\n\n    if len(merges) &gt; 1:\n        # suppress pandas future warning occuring within xarray.concat\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n            # if sims are named, combine_by_coord returns a dataset\n            res = xr.combine_by_coords([m.rename(None) for m in merges])\n    else:\n        res = merge\n\n    res = si_utils.get_sim_from_xim(res)\n    si_utils.set_sim_affine(\n        res,\n        param_utils.identity_transform(len(sdims)),\n        transform_key,\n    )\n\n    # order channels in the same way as first input sim\n    # (combine_by_coords may change coordinate order)\n    if \"c\" in res.dims:\n        res = res.sel({\"c\": sims[0].coords[\"c\"].values})\n\n    return res\n</code></pre>"},{"location":"api/registration/","title":"Registration","text":""},{"location":"api/registration/#multiview_stitcher.registration.register","title":"<code>register(msims, transform_key=None, reg_channel_index=None, reg_channel=None, new_transform_key=None, registration_binning=None, reg_res_level=None, overlap_tolerance=0.0, pairwise_reg_func=phase_correlation_registration, pairwise_reg_func_kwargs=None, groupwise_resolution_method='global_optimization', groupwise_resolution_kwargs=None, pre_registration_pruning_method='alternating_pattern', pre_reg_pruning_method_kwargs=None, post_registration_do_quality_filter=False, post_registration_quality_threshold=0.2, plot_summary=False, pairs=None, scheduler=None, n_parallel_pairwise_regs=None, return_dict=False)</code>","text":"<p>Register a list of views to a common extrinsic coordinate system.</p> <p>This function is the main entry point for registration.</p> <p>1) Build a graph of pairwise overlaps between views 2) Determine registration pairs from this graph 3) Register each pair of views.    Need to add option to pass registration functions here. 4) Determine the parameters mapping each view into the new extrinsic    coordinate system.    Currently done by determining a reference view and concatenating for reach    view the pairwise transforms along the shortest paths towards the ref view.</p>"},{"location":"api/registration/#multiview_stitcher.registration.register--parameters","title":"Parameters","text":"<p>msims : list of MultiscaleSpatialImage     Input views reg_channel_index : int, optional     Index of channel to be used for registration, by default None reg_channel : str, optional     Name of channel to be used for registration, by default None     Overrides reg_channel_index transform_key : str, optional     Extrinsic coordinate system to use as a starting point     for the registration, by default None new_transform_key : str, optional     If set, the registration result will be registered as a new extrinsic     coordinate system in the input views (with the given name), by default None registration_binning : dict, optional     Binning applied to each dimension during registration, by default None.     If reg_res_level is also provided, the binning factors must be compatible      with the resolution level. reg_res_level : int, optional     Resolution level to use for registration (e.g., 0 for scale0, 1 for scale1).     If None and registration_binning is provided, the optimal resolution level      is automatically determined. By default None. overlap_tolerance : float or dict, optional     Extend overlap regions considered for pairwise registration.     - if 0, the overlap region is the intersection of the bounding boxes.     - if &gt; 0, the overlap region is the intersection of the bounding boxes         extended by this value in all spatial dimensions.     - if None, the full images are used for registration pairwise_reg_func : Callable, optional     Function used for registration. See the docs for the function API.     By default, phase_correlation_registration is used. Another useful built-in     registration function is <code>pairwise_reg_func=registration.registration_ANTsPy</code>     for translation, rigid, similarity or affine registration using ANTsPy. pairwise_reg_func_kwargs : dict, optional     Additional keyword arguments passed to the registration function.     In the case of <code>pairwise_reg_func=registration_ANTsPy</code>, this can include e.g:     - 'transform_type': ['Translation', 'Rigid' 'Affine'] or ['Similarity']     For further parameters, see the docstring of the registration function. groupwise_resolution_method : str, optional     Method used to determine the final transform parameters     from pairwise registrations:     - 'global_optimization': global optimization considering all pairwise transforms     - 'shortest_paths': concatenation of pairwise transforms along shortest paths groupwise_resolution_kwargs : dict, optional     Additional keyword arguments passed to the groupwise optimization function.     IMPORTANT: The \"transform\" key here determines the final types of transformations.     By default this is {'transform': 'translation'}. Even if the pairwise registration     function returns a rigid (rotation) or affine transformation, the groupwise resolution     will restrict the final transformations to the indicated type (i.e. for a pairwise     registration function that returns a rigid transformation, it most cases it makes sense     to use 'rigid' as the groupwise resolution transform type).     Most important parameters:     - 'transform': str         Type of transformation to use for groupwise resolution.         Available types:         - 'translation': translation (default)         - 'rigid': rigid body transformation         - 'similarity': similarity transformation         - 'affine': affine transformation     - 'abs_tol': float, optional         Absolute value of max edge residual below which the groupwise resolution stops.     - 'reference_view': int, optional         Reference view which keeps its transformation fixed. pre_registration_pruning_method : str, optional     Method used to eliminate registration edges (e.g. diagonals) from the view adjacency     graph before registration. Available methods:     - None: No pruning, useful when no regular arrangement is present.     - 'alternating_pattern': Prune to edges between squares of differering         colors in checkerboard pattern. Useful for regular 2D tile arrangements (of both 2D or 3D data).     - 'shortest_paths_overlap_weighted': Prune to shortest paths in overlap graph         (weighted by overlap). Useful to minimize the number of pairwise registrations.     - 'otsu_threshold_on_overlap': Prune to edges with overlap above Otsu threshold.         This is useful for regular 2D or 3D grid arrangements, as diagonal edges will be pruned.     - 'keep_axis_aligned': Keep only edges that align with tile axes. This is useful for regular grid         arrangements and to explicitely prune diagonals, e.g. when other methods fail. pre_reg_pruning_method_kwargs : dict, optional     Additional keyword arguments passed to the pre-registration pruning method, e.g.     - 'keep_axis_aligned': 'max_angle' (larger angles between stack axis and pair edge are discarded, default 0.2) post_registration_do_quality_filter : bool, optional post_registration_quality_threshold : float, optional     Threshold used to filter edges by quality after registration,     by default None (no filtering) plot_summary : bool, optional     If True (and <code>new_transform_key</code> is set), plot graphs summarising the registration process and results:     1) Cross correlation values of pairwise registrations        (stack boundaries shown as before registration)     2) Residual distances between registration edges after global parameter resolution.        Grey edges have been removed during glob param res (stack boundaries shown as after registration).     Stack boundary positions reflect the registration result.     By default False pairs : list of tuples, optional     If set, initialises the view adjacency graph using the indicates     pairs of view/tile indices, by default None scheduler : str, optional     (Deprecated since &gt;0.1.28) Dask scheduler to use for parallel computation, by default None     This parameter is deprecated and no longer used.     Use a context manager instead to set the dask scheduler used within register(), e.g.     <code>with dask.config.set(scheduler='threads'): register(...)</code> n_parallel_pairwise_regs : int, optional     Number of parallel pairwise registrations to run. Setting this is specifically     useful for limiting memory usage.     By default None (all pairwise registrations are run in parallel) return_dict : bool, optional     If True, return a dict containing params, registration metrics and more, by default False</p>"},{"location":"api/registration/#multiview_stitcher.registration.register--returns","title":"Returns","text":"<p>list of xr.DataArray     Parameters mapping each view into a new extrinsic coordinate system or dict     Dictionary containing the following keys:     - 'params': Parameters mapping each view into a new extrinsic coordinate system     - 'pairwise_registration': Dictionary containing the following         - 'summary_plot': Tuple containing the figure and axis of the summary plot         - 'graph': networkx graph of pairwise registrations         - 'metrics': Dictionary containing the following metrics:             - 'qualities': Edge registration qualities     - 'groupwise_resolution': Dictionary containing the following         - 'summary_plot': Tuple containing the figure and axis of the summary plot         - 'graph': networkx graph of groupwise resolution         - 'metrics': Dictionary containing the following metrics:             - 'residuals': Edge residuals after groupwise resolution</p> Source code in <code>src/multiview_stitcher/registration.py</code> <pre><code>def register(\n    msims: list[MultiscaleSpatialImage],\n    transform_key: str = None,\n    reg_channel_index: int = None,\n    reg_channel: str = None,\n    new_transform_key: str = None,\n    registration_binning: dict[str, int] = None,\n    reg_res_level: int = None,\n    overlap_tolerance: Union[float, dict[str, float]] = 0.0,\n    pairwise_reg_func=phase_correlation_registration,\n    pairwise_reg_func_kwargs: dict = None,\n    groupwise_resolution_method=\"global_optimization\",\n    groupwise_resolution_kwargs: dict = None,\n    pre_registration_pruning_method=\"alternating_pattern\",\n    pre_reg_pruning_method_kwargs: dict = None,\n    post_registration_do_quality_filter: bool = False,\n    post_registration_quality_threshold: float = 0.2,\n    plot_summary: bool = False,\n    pairs: list[tuple[int, int]] = None,\n    scheduler=None,  # deprecated, see docstring\n    n_parallel_pairwise_regs: int = None,\n    return_dict: bool = False,\n):\n    \"\"\"\n\n    Register a list of views to a common extrinsic coordinate system.\n\n    This function is the main entry point for registration.\n\n    1) Build a graph of pairwise overlaps between views\n    2) Determine registration pairs from this graph\n    3) Register each pair of views.\n       Need to add option to pass registration functions here.\n    4) Determine the parameters mapping each view into the new extrinsic\n       coordinate system.\n       Currently done by determining a reference view and concatenating for reach\n       view the pairwise transforms along the shortest paths towards the ref view.\n\n    Parameters\n    ----------\n    msims : list of MultiscaleSpatialImage\n        Input views\n    reg_channel_index : int, optional\n        Index of channel to be used for registration, by default None\n    reg_channel : str, optional\n        Name of channel to be used for registration, by default None\n        Overrides reg_channel_index\n    transform_key : str, optional\n        Extrinsic coordinate system to use as a starting point\n        for the registration, by default None\n    new_transform_key : str, optional\n        If set, the registration result will be registered as a new extrinsic\n        coordinate system in the input views (with the given name), by default None\n    registration_binning : dict, optional\n        Binning applied to each dimension during registration, by default None.\n        If reg_res_level is also provided, the binning factors must be compatible \n        with the resolution level.\n    reg_res_level : int, optional\n        Resolution level to use for registration (e.g., 0 for scale0, 1 for scale1).\n        If None and registration_binning is provided, the optimal resolution level \n        is automatically determined. By default None.\n    overlap_tolerance : float or dict, optional\n        Extend overlap regions considered for pairwise registration.\n        - if 0, the overlap region is the intersection of the bounding boxes.\n        - if &gt; 0, the overlap region is the intersection of the bounding boxes\n            extended by this value in all spatial dimensions.\n        - if None, the full images are used for registration\n    pairwise_reg_func : Callable, optional\n        Function used for registration. See the docs for the function API.\n        By default, phase_correlation_registration is used. Another useful built-in\n        registration function is `pairwise_reg_func=registration.registration_ANTsPy`\n        for translation, rigid, similarity or affine registration using ANTsPy.\n    pairwise_reg_func_kwargs : dict, optional\n        Additional keyword arguments passed to the registration function.\n        In the case of `pairwise_reg_func=registration_ANTsPy`, this can include e.g:\n        - 'transform_type': ['Translation', 'Rigid' 'Affine'] or ['Similarity']\n        For further parameters, see the docstring of the registration function.\n    groupwise_resolution_method : str, optional\n        Method used to determine the final transform parameters\n        from pairwise registrations:\n        - 'global_optimization': global optimization considering all pairwise transforms\n        - 'shortest_paths': concatenation of pairwise transforms along shortest paths\n    groupwise_resolution_kwargs : dict, optional\n        Additional keyword arguments passed to the groupwise optimization function.\n        IMPORTANT: The \"transform\" key here determines the final types of transformations.\n        By default this is {'transform': 'translation'}. Even if the pairwise registration\n        function returns a rigid (rotation) or affine transformation, the groupwise resolution\n        will restrict the final transformations to the indicated type (i.e. for a pairwise\n        registration function that returns a rigid transformation, it most cases it makes sense\n        to use 'rigid' as the groupwise resolution transform type).\n        Most important parameters:\n        - 'transform': str\n            Type of transformation to use for groupwise resolution.\n            Available types:\n            - 'translation': translation (default)\n            - 'rigid': rigid body transformation\n            - 'similarity': similarity transformation\n            - 'affine': affine transformation\n        - 'abs_tol': float, optional\n            Absolute value of max edge residual below which the groupwise resolution stops.\n        - 'reference_view': int, optional\n            Reference view which keeps its transformation fixed.\n    pre_registration_pruning_method : str, optional\n        Method used to eliminate registration edges (e.g. diagonals) from the view adjacency\n        graph before registration. Available methods:\n        - None: No pruning, useful when no regular arrangement is present.\n        - 'alternating_pattern': Prune to edges between squares of differering\n            colors in checkerboard pattern. Useful for regular 2D tile arrangements (of both 2D or 3D data).\n        - 'shortest_paths_overlap_weighted': Prune to shortest paths in overlap graph\n            (weighted by overlap). Useful to minimize the number of pairwise registrations.\n        - 'otsu_threshold_on_overlap': Prune to edges with overlap above Otsu threshold.\n            This is useful for regular 2D or 3D grid arrangements, as diagonal edges will be pruned.\n        - 'keep_axis_aligned': Keep only edges that align with tile axes. This is useful for regular grid\n            arrangements and to explicitely prune diagonals, e.g. when other methods fail.\n    pre_reg_pruning_method_kwargs : dict, optional\n        Additional keyword arguments passed to the pre-registration pruning method, e.g.\n        - 'keep_axis_aligned': 'max_angle' (larger angles between stack axis and pair edge are discarded, default 0.2)\n    post_registration_do_quality_filter : bool, optional\n    post_registration_quality_threshold : float, optional\n        Threshold used to filter edges by quality after registration,\n        by default None (no filtering)\n    plot_summary : bool, optional\n        If True (and `new_transform_key` is set), plot graphs summarising the registration process and results:\n        1) Cross correlation values of pairwise registrations\n           (stack boundaries shown as before registration)\n        2) Residual distances between registration edges after global parameter resolution.\n           Grey edges have been removed during glob param res (stack boundaries shown as after registration).\n        Stack boundary positions reflect the registration result.\n        By default False\n    pairs : list of tuples, optional\n        If set, initialises the view adjacency graph using the indicates\n        pairs of view/tile indices, by default None\n    scheduler : str, optional\n        (Deprecated since &gt;0.1.28) Dask scheduler to use for parallel computation, by default None\n        This parameter is deprecated and no longer used.\n        Use a context manager instead to set the dask scheduler used within register(), e.g.\n        `with dask.config.set(scheduler='threads'): register(...)`\n    n_parallel_pairwise_regs : int, optional\n        Number of parallel pairwise registrations to run. Setting this is specifically\n        useful for limiting memory usage.\n        By default None (all pairwise registrations are run in parallel)\n    return_dict : bool, optional\n        If True, return a dict containing params, registration metrics and more, by default False\n\n    Returns\n    -------\n    list of xr.DataArray\n        Parameters mapping each view into a new extrinsic coordinate system\n    or\n    dict\n        Dictionary containing the following keys:\n        - 'params': Parameters mapping each view into a new extrinsic coordinate system\n        - 'pairwise_registration': Dictionary containing the following\n            - 'summary_plot': Tuple containing the figure and axis of the summary plot\n            - 'graph': networkx graph of pairwise registrations\n            - 'metrics': Dictionary containing the following metrics:\n                - 'qualities': Edge registration qualities\n        - 'groupwise_resolution': Dictionary containing the following\n            - 'summary_plot': Tuple containing the figure and axis of the summary plot\n            - 'graph': networkx graph of groupwise resolution\n            - 'metrics': Dictionary containing the following metrics:\n                - 'residuals': Edge residuals after groupwise resolution\n    \"\"\"\n\n    # warn about deprecated parameter\n    if scheduler is not None:\n        warnings.warn(\n            \"The register(..., scheduler=) parameter is deprecated, no longer used \"\n            \"and will be removed in a future version. \"\n            \"Use a context manager to set the dask scheduler used within register(), e.g. \"\n            \"`with dask.config.set(scheduler='threads'): register(...)`\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    if pairwise_reg_func_kwargs is None:\n        pairwise_reg_func_kwargs = {}\n\n    if groupwise_resolution_kwargs is None:\n        groupwise_resolution_kwargs = {}\n\n    if pre_reg_pruning_method_kwargs is None:\n        pre_reg_pruning_method_kwargs = {}\n\n    sims = [msi_utils.get_sim_from_msim(msim) for msim in msims]\n\n    if \"c\" in msi_utils.get_dims(msims[0]):\n        if reg_channel is None:\n            if reg_channel_index is None:\n                for msim in msims:\n                    if \"c\" in msi_utils.get_dims(msim):\n                        raise (\n                            Exception(\"Please choose a registration channel.\")\n                        )\n            else:\n                reg_channel = sims[0].coords[\"c\"][reg_channel_index]\n\n        msims_reg = [\n            msi_utils.multiscale_sel_coords(msim, {\"c\": reg_channel})\n            if \"c\" in msi_utils.get_dims(msim)\n            else msim\n            for imsim, msim in enumerate(msims)\n        ]\n    else:\n        msims_reg = msims\n\n    g = mv_graph.build_view_adjacency_graph_from_msims(\n        msims_reg,\n        transform_key=transform_key,\n        pairs=pairs,\n        overlap_tolerance=overlap_tolerance,\n    )\n\n    if pre_registration_pruning_method is not None:\n        g_reg = mv_graph.prune_view_adjacency_graph(\n            g,\n            method=pre_registration_pruning_method,\n            pruning_method_kwargs=pre_reg_pruning_method_kwargs,\n        )\n    else:\n        g_reg = g\n\n    g_reg_computed = compute_pairwise_registrations(\n        msims_reg,\n        g_reg,\n        transform_key=transform_key,\n        registration_binning=registration_binning,\n        reg_res_level=reg_res_level,\n        overlap_tolerance=overlap_tolerance,\n        pairwise_reg_func=pairwise_reg_func,\n        pairwise_reg_func_kwargs=pairwise_reg_func_kwargs,\n        n_parallel_pairwise_regs=n_parallel_pairwise_regs,\n    )\n\n    if post_registration_do_quality_filter:\n        # filter edges by quality\n        g_reg_computed = mv_graph.filter_edges(\n            g_reg_computed,\n            threshold=post_registration_quality_threshold,\n            weight_key=\"quality\",\n        )\n\n    params, groupwise_resolution_info_dict = groupwise_resolution(\n        g_reg_computed,\n        method=groupwise_resolution_method,\n        **groupwise_resolution_kwargs,\n    )\n\n    params = [params[iview] for iview in sorted(g_reg_computed.nodes())]\n\n    if new_transform_key is not None:\n        for imsim, msim in enumerate(msims):\n            msi_utils.set_affine_transform(\n                msim,\n                params[imsim],\n                transform_key=new_transform_key,\n                base_transform_key=transform_key,\n            )\n\n        if plot_summary or return_dict:\n            edges = list(g_reg_computed.edges())\n            fig_pair_reg, ax_pair_reg = vis_utils.plot_positions(\n                msims,\n                transform_key=transform_key,\n                edges=edges,\n                edge_color_vals=np.array(\n                    [\n                        g_reg_computed.get_edge_data(*e)[\"quality\"].mean()\n                        for e in edges\n                    ]\n                ),\n                edge_label=\"Pairwise view correlation\",\n                display_view_indices=True,\n                use_positional_colors=False,\n                plot_title=\"Pairwise registration summary\",\n                show_plot=plot_summary,\n            )\n\n            if groupwise_resolution_info_dict is not None:\n                edge_residuals = np.array(\n                    [\n                        groupwise_resolution_info_dict[\n                            \"optimized_graph_t0\"\n                        ].get_edge_data(*e)[\"residual\"]\n                        if e\n                        in groupwise_resolution_info_dict[\n                            \"optimized_graph_t0\"\n                        ].edges\n                        else np.nan\n                        for e in edges\n                    ]\n                )\n                edge_clims = [\n                    np.nanmin(edge_residuals),\n                    np.nanmax(edge_residuals),\n                ]\n                if edge_clims[0] == edge_clims[1]:\n                    edge_clims = [0, 1]\n                fig_group_res, ax_group_res = vis_utils.plot_positions(\n                    msims,\n                    transform_key=new_transform_key,\n                    edges=edges,\n                    edge_color_vals=edge_residuals,\n                    edge_cmap=\"Spectral_r\",\n                    edge_clims=edge_clims,\n                    edge_label=\"Remaining edge residuals [distance units]\",\n                    display_view_indices=True,\n                    use_positional_colors=False,\n                    plot_title=\"Global parameter resolution summary\",\n                    show_plot=plot_summary,\n                )\n            else:\n                fig_group_res, ax_group_res = None, None\n    else:\n        fig_pair_reg, ax_pair_reg, fig_group_res, ax_group_res = [\n            None,\n            None,\n            None,\n            None,\n        ]\n\n    if return_dict:\n        # limit output graphs to first timepoint (for now)\n        g_reg_computed = g_reg_computed.copy()\n        for e in g_reg_computed.edges:\n            for k, v in g_reg_computed.edges[e].items():\n                if isinstance(v, xr.DataArray) and \"t\" in v.coords:\n                    g_reg_computed.edges[e][k] = g_reg_computed.edges[e][\n                        k\n                    ].sel({\"t\": g_reg_computed.edges[e][k].coords[\"t\"][0]})\n\n        return {\n            \"params\": params,\n            \"pairwise_registration\": {\n                \"summary_plot\": (fig_pair_reg, ax_pair_reg),\n                \"graph\": g_reg_computed,\n                \"metrics\": {\n                    \"qualities\": nx.get_edge_attributes(\n                        g_reg_computed, \"quality\"\n                    ),\n                },\n            },\n            \"groupwise_resolution\": {\n                \"summary_plot\": (fig_group_res, ax_group_res),\n                \"graph\": groupwise_resolution_info_dict[\"optimized_graph_t0\"],\n                \"metrics\": {\n                    \"residuals\": nx.get_edge_attributes(\n                        groupwise_resolution_info_dict[\"optimized_graph_t0\"],\n                        \"residual\",\n                    ),\n                },\n            },\n        }\n    else:\n        return params\n</code></pre>"}]}