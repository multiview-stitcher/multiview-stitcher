{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c09c7b",
   "metadata": {},
   "source": [
    "# mesoSPIM example workflow\n",
    "\n",
    "## Intro\n",
    "\n",
    "This notebook stitches an example mesoSPIM dataset comprised of:\n",
    "- four tiles (2x2 grid)\n",
    "- two channels\n",
    "- two arms (illuminations from opposite sides)\n",
    "\n",
    "The data is available via Globus [here](https://app.globus.org/file-manager?origin_id=b91c61e8-5611-4970-bf4c-9b0d392e5c3c&origin_path=%2FReussMouseBrain-2x2Tiles-2Ch-2Arms%2F). The dataset was shared by Nikita Vladimirov (see image.sc thread [here](https://forum.image.sc/t/are-there-publicly-available-mesospim-datasets-unprocessed/116020/14)).\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Download the dataset to your local machine.\n",
    "1. Setup Python in your preferred way (e.g. conda, venv, pipenv, poetry, etc.)\n",
    "1. Make sure `multiview-stitcher >= 0.1.35` is installed: `pip install \"multiview-stitcher>=0.1.35\"`\n",
    "1. The dataset is in BigStitcher OME-Zarr format. For reading the associated BigStitcher XML file, we use the `pydantic-bigstitcher` package. Install it via pip ```pip install pydantic-bigstitcher```\n",
    "1. Optionally, install `ray` for parallelising fusion on top of dask: `pip install \"ray[default]\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2ea13",
   "metadata": {},
   "source": [
    "## Indicate path to BigStitcher XML file\n",
    "\n",
    "Note: Here we're reading the positional / transform metadata from a predefined metadata file. Check out the example notebooks [here] to see how this can be set manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c392203",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = \"/Users/albertm/software/multiview-stitcher/image-datasets/mesospim/OME-ZARR/2x2-tiles_ome-zarr.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9031c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "import pydantic_bigstitcher as pbs\n",
    "\n",
    "from multiview_stitcher import spatial_image_utils as si_utils\n",
    "from multiview_stitcher import (\n",
    "    registration,\n",
    "    fusion,\n",
    "    param_utils,\n",
    "    msi_utils,\n",
    "    misc_utils,\n",
    "    vis_utils,\n",
    "    ngff_utils,\n",
    ")\n",
    "\n",
    "# interactive visualization in jupyter notebooks\n",
    "%matplotlib ipympl\n",
    "\n",
    "# mesospim example data contains \"discrete\" parameter in axes metadata\n",
    "# which is not part of the NGFF spec and causes ngff-zarr to throw an error\n",
    "# thus we need to monkey patch ngff-zarr to ignore this parameter\n",
    "\n",
    "# monkeypatch: monkey patch __init__ of `ngff_zarr.Axis` to avoid throwing error with unexpected \"discrete\" parameter\n",
    "import ngff_zarr\n",
    "_original_init = ngff_zarr.Axis.__init__\n",
    "ngff_zarr.Axis.__init__ = lambda self, *args, **kwargs: _original_init(\n",
    "    self, *args, **{k: v for k, v in kwargs.items() if k != \"discrete\"}\n",
    ")  # Ignore unexpected kwargs like \"discrete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22246e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions help\n",
    "# - reading the transform metadata from BigStitcher XML files and\n",
    "# - converting them to the xparams format used in multiview-stitcher\n",
    "\n",
    "def transform_to_xparams(transform):\n",
    "    matrix = transform.affine\n",
    "    offset = transform.translation\n",
    "\n",
    "    dims = ['z', 'y', 'x']\n",
    "    affine = param_utils.affine_from_linear_affine(\n",
    "        np.array([matrix[dim1][dim2] for dim1 in dims for dim2 in dims] + [offset[dim] for dim in dims])\n",
    "    )\n",
    "    xaffine = param_utils.affine_to_xaffine(affine)\n",
    "\n",
    "    return xaffine\n",
    "\n",
    "def view_transforms_to_xparams(view_transforms, n_transforms_to_consider=None):\n",
    "    xparamss = [transform_to_xparams(view_transform.to_transform().transform)\n",
    "                # for view_transform in view_transforms[:-1]]\n",
    "                for view_transform in view_transforms[slice(n_transforms_to_consider)]]\n",
    "    \n",
    "    xparams = xparamss[0]\n",
    "    for xp in xparamss[1:]:\n",
    "        xparams = param_utils.matmul_xparams(xparams, xp)\n",
    "    \n",
    "    return xparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50ddb6",
   "metadata": {},
   "source": [
    "## Read the data and define the initial tile configuration\n",
    "\n",
    "Note that no actual pixel data is loaded yet. This only reads the metadata and sets up the initial tile configuration.\n",
    "\n",
    "Mini glossary:\n",
    "- `msim`: Multi-scale image: xr.DataTree containing multiple xr.DataArray at different resolution levels\n",
    "- `xparams`: affine parameters represented as xarray.DataArray, which allows to label the axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "xml_path = Path(xml_path) # defined above\n",
    "\n",
    "# use pydantic-bigstitcher to parse the XML file\n",
    "sd = pbs.SpimData2.from_xml(open(xml_path).read())\n",
    "zgroups = sd.sequence_description.image_loader.zgroups.elements\n",
    "\n",
    "df = []\n",
    "# for each zarr group (i.e. each view), read the metadata and create a spatial image\n",
    "for izg, zgroup in enumerate(zgroups[:]):\n",
    "    \n",
    "    # read basic metadata from XML file\n",
    "    filepath = xml_path.parent / sd.sequence_description.image_loader.zarr.path / zgroup.path\n",
    "    view_setup = sd.sequence_description.view_setups.elements[izg]\n",
    "    ch = view_setup.attributes.channel\n",
    "    ill = view_setup.attributes.illumination\n",
    "    tile = view_setup.attributes.tile\n",
    "    angle = view_setup.attributes.angle\n",
    "    \n",
    "    spacing = {dim: float(v)\n",
    "               for dim, v in zip([\"x\", \"y\", \"z\"], view_setup.voxel_size.size.split(\" \"))}\n",
    "\n",
    "    xparams_ome_zarr = view_transforms_to_xparams(\n",
    "        sd.view_registrations.elements[izg].view_transforms, n_transforms_to_consider=1)\n",
    "\n",
    "    # In this dataset the second transform in the XML file defines the anisotropic pixel spacing,\n",
    "    # which is not contained in the OME-Zarr metadata.\n",
    "    # We need this only for correct visualization in neuroglancer (which reads the spacing\n",
    "    # from OME-Zarr metadata)\n",
    "    xparams_xml = view_transforms_to_xparams(\n",
    "        sd.view_registrations.elements[izg].view_transforms, n_transforms_to_consider=None)\n",
    "\n",
    "    # somehow params are in pixel uits    \n",
    "    for idim, dim in enumerate(['z', 'y', 'x']):\n",
    "        xparams_ome_zarr.loc[dim, \"1\"] *= spacing[dim]\n",
    "        xparams_xml.loc[dim, \"1\"] *= spacing[dim]\n",
    "\n",
    "    msim = ngff_utils.ngff_multiscales_to_msim(\n",
    "        ngff_zarr.from_ngff_zarr(filepath),\n",
    "        transform_key='ome-zarr',\n",
    "        )\n",
    "    \n",
    "    msim = msim.map_over_datasets(lambda ds: xr.Dataset(\n",
    "        {'image': ds.image.assign_coords(\n",
    "                    {dim: ds.image.coords[dim] * spacing[dim]\n",
    "                            for dim in ['z', 'y', 'x']} | \\\n",
    "                    {'c': [ch]}\n",
    "                    )} | \\\n",
    "        {t: ds.data_vars[t] for t in ds.data_vars if t != 'image'})#.assign_coords({'v': izg})\n",
    "        if len(ds.data_vars) > 0 else ds)\n",
    "    \n",
    "    msi_utils.set_affine_transform(msim, xparams_ome_zarr, transform_key='ome-zarr')\n",
    "    msi_utils.set_affine_transform(msim, xparams_xml, transform_key='xml')\n",
    "\n",
    "    df.append(\n",
    "        {\n",
    "            'msim': msim,\n",
    "            'ch': int(ch),\n",
    "            'ill': int(ill),\n",
    "            'tile': int(tile),\n",
    "            'angle': int(angle),\n",
    "            'filepath': filepath,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c6e5c",
   "metadata": {},
   "source": [
    "## Create a pandas DataFrame describing the referenced image stacks\n",
    "\n",
    "Having the data in a pandas DataFrame makes it easy to filter and select subsets (e.g. channels, tiles, illuminations) of the data.\n",
    "\n",
    "**Note**: Typically in multiview-stitcher workflows we combine the channels of a given tile within the same \"spatial-image\" or \"multiscale-spatial-image\". However, here we're going to keep the channels separate because this is more convenient when using neuroglancer for visualization (since neuroglancer reads the data directly from the files). Therefore an alternative would have been to resave the data the combined data as OME-Zarr with combined channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac4168",
   "metadata": {},
   "source": [
    "## Visualize the tile configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df92385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize view configuration\n",
    "\n",
    "# make a subselection so that the plot is not too crowded\n",
    "df_vis = df[(df.ch == 0) & (df.ill == 0)]\n",
    "\n",
    "vis_utils.plot_positions(\n",
    "    df_vis[\"msim\"].tolist(), transform_key='xml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab209f3c",
   "metadata": {},
   "source": [
    "## Examine the multi-scale image data\n",
    "\n",
    "Let's look at the data sizes for the different scales. This will help us to decide which scale to use for illumination selection and registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data sizes for the different scales\n",
    "print(\"Data sizes for different scales (first tile):\")\n",
    "msim = df['msim'][0]\n",
    "print('Dimensions:', msim['scale0/image'].dims)\n",
    "for scale in msim:\n",
    "    print(f\"Scale {scale}: {msim[scale].image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249bc8e",
   "metadata": {},
   "source": [
    "## Visualize the input data using neuroglancer\n",
    "\n",
    "A browser window should open. When finished viewing the Jupyter Notebook kernel needs to be interrupted. This is because the cell below starts a web server that serves the data to neuroglancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which illumination and channel to visualize\n",
    "# It's possible to view all of these, but the neuroglancer view will become crowded.\n",
    "# This neuroglancer interaction is to be improved in the future\n",
    "# (and is already better when working with OME-Zarr files that contain all channels)\n",
    "df_vis = df[(df.ch == 1) & (df.ill == 0)]\n",
    "\n",
    "import importlib\n",
    "importlib.reload(vis_utils)\n",
    "\n",
    "vis_utils.view_neuroglancer(\n",
    "    ome_zarr_paths=[str(fp) for fp in df_vis['filepath'].tolist()],\n",
    "    sims=[msi_utils.get_sim_from_msim(msim) for msim in df_vis['msim']],\n",
    "    transform_key='xml',\n",
    "    # transform_key='ome-zarr',\n",
    "    contrast_limits=(0, 500),\n",
    "    single_layer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fa979",
   "metadata": {},
   "source": [
    "## Illumination selection\n",
    "\n",
    "Here we decide on which illumination to keep by calculating gradient magnitude to find the most informative views (inspired by BigStitcher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3491f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that can be applied to each group of tile, angle, ch\n",
    "def select_illumination(rows, scale):\n",
    "\n",
    "    # calculate sum of gradient magnitudes across each channel\n",
    "    grad_mags = []\n",
    "    for msim in rows['msim']:\n",
    "        daims = np.linalg.norm(\n",
    "            np.gradient(msim[scale].image.data.squeeze())\n",
    "            , axis=0\n",
    "        )\n",
    "        grad_mag = np.mean(daims)\n",
    "        grad_mags.append(grad_mag)\n",
    "\n",
    "    # determine the illumination with the highest gradient magnitude\n",
    "    grad_mags = da.compute(grad_mags)[0]\n",
    "    msim_index_to_select = np.argmax(grad_mags)\n",
    "\n",
    "    print(f\"Gradient magnitudes: {[float(gm) for gm in grad_mags]}\")\n",
    "    print(f\"Selecting illumination {msim_index_to_select}\")\n",
    "\n",
    "    return rows.iloc[msim_index_to_select]\n",
    "\n",
    "# apply the function to each group of tile, angle, ch\n",
    "dfi = df.groupby(['tile', 'angle', 'ch'])[['msim', 'ill']].apply(\n",
    "    select_illumination,\n",
    "    scale='scale3' # resolution level on which to calculate gradient magnitudes\n",
    ").reset_index()\n",
    "\n",
    "# merge with original dataframe to recover all columns\n",
    "dfi = dfi.merge(df, on=['tile', 'angle', 'ch', 'ill'], suffixes=('_', ''))\n",
    "\n",
    "dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcde52a",
   "metadata": {},
   "source": [
    "## Registration\n",
    "\n",
    "Here, we'll use phase correlation based registration to register the tiles.\n",
    "\n",
    "Note that this step (currently) requires that at least the overlap of two neighboring tiles needs to fit into memory.\n",
    "\n",
    "How to achieve this?\n",
    "- choose a suitable scale to register on\n",
    "- advanced: Define a [custom registration function](https://multiview-stitcher.github.io/multiview-stitcher/main/extension_api_pairwise_registration/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.diagnostics\n",
    "\n",
    "# select a channel for registration\n",
    "reg_channel_index = 1\n",
    "df_reg = dfi[(dfi.ch == reg_channel_index)]\n",
    "\n",
    "# select a resolution level for registration\n",
    "reg_res_level = 2\n",
    "\n",
    "with dask.diagnostics.ProgressBar():\n",
    "    registration.register(\n",
    "            df_reg['msim'].tolist(),\n",
    "            transform_key='ome-zarr',\n",
    "            new_transform_key='phase_corr_registered',\n",
    "            reg_channel_index=0,\n",
    "            # registration_binning={'z': 2, 'y': 2, 'x': 2},\n",
    "            reg_res_level=reg_res_level,\n",
    "            n_parallel_pairwise_regs=4, # trade-off speed vs memory requirements (estimate of required memory: 2 * n_parallel_pairwise_regs * overlap_data_size))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9eb77",
   "metadata": {},
   "source": [
    "## Visualize registration result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis = df_reg\n",
    "\n",
    "vis_utils.view_neuroglancer(\n",
    "    ome_zarr_paths=[str(fp) for fp in df_vis['filepath'].tolist()],\n",
    "    sims=[msi_utils.get_sim_from_msim(msim) for msim in df_vis['msim']],\n",
    "    transform_key='phase_corr_registered',\n",
    "    contrast_limits=(0, 500),\n",
    "    single_layer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6bb0f",
   "metadata": {},
   "source": [
    "## Combine channels\n",
    "\n",
    "Combine stacks from different channels into a single stack.\n",
    "\n",
    "In doing so, the registration transforms obtained in the previous step are copied from the registered channel to the other channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e100583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to combine msims along a given dimension\n",
    "def combine_msims_along_dim(msims, concat_kwargs={}, dim='c'):\n",
    "\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        return xr.DataTree.from_dict(\n",
    "            {sk: xr.concat([msim[sk].dataset for msim in msims],\n",
    "                    dim=dim, data_vars='different', **concat_kwargs) for sk in list(msims)[0].keys()}\n",
    "        )\n",
    "\n",
    "# merge channels\n",
    "dfic = dfi.groupby(['tile', 'angle'])['msim'].apply(\n",
    "    combine_msims_along_dim, dim='c'\n",
    ").reset_index()\n",
    "\n",
    "dfic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9212cde",
   "metadata": {},
   "source": [
    "## Fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(fusion)\n",
    "\n",
    "# Define output Zarr URL\n",
    "# make sure to set an unexisting / unused output path\n",
    "output_zarr_url = \"fused_mesospim.zarr\"\n",
    "\n",
    "# define which transform key to use for fusion\n",
    "# fusion_transform_key = 'ome-zarr' # fuse without alignment\n",
    "fusion_transform_key = 'phase_corr_registered' # fuse with alignment\n",
    "\n",
    "# msims = dfic['msim'].tolist()\n",
    "\n",
    "msims = df_reg['msim'].tolist()\n",
    "sims = [msi_utils.get_sim_from_msim(\n",
    "    msim,\n",
    "    scale='scale2' # resolution level to fuse\n",
    "    ) for msim in msims]\n",
    "\n",
    "fused = fusion.fuse(\n",
    "    sims=sims,\n",
    "    transform_key=fusion_transform_key,\n",
    "    output_chunksize={dim: 256 for dim in ['z', 'y', 'x']},\n",
    "    blending_widths={\"z\": 1000, \"y\": 1000, \"x\": 1000}, # in microns\n",
    "    # overlap_in_pixels can be left to default; blending widths handle boundary smoothing\n",
    "    output_zarr_url=output_zarr_url,\n",
    "    zarr_options={\n",
    "        \"ome_zarr\": True,\n",
    "        # \"ngff_version\": \"0.4\",  # optional\n",
    "    },\n",
    "    # optionally, we can use ray for parallelization (`pip install \"ray[default]\"`)\n",
    "    # batch_options={\n",
    "    #     \"batch_func\": misc_utils.process_batch_using_ray,\n",
    "    #     \"n_batch\": 4, # number of chunk fusions to schedule / submit at a time\n",
    "    #     \"batch_func_kwargs\": {\n",
    "    #         'num_cpus': 4 # number of processes for parallel processing to use with ray\n",
    "    #     },\n",
    "    # },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af04f62b",
   "metadata": {},
   "source": [
    "## Visualize fused result using neuroglancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interrupt the notebook cell to stop the viewer\n",
    "vis_utils.view_neuroglancer(\n",
    "    sims=[fused],\n",
    "    ome_zarr_paths=[output_zarr_url],\n",
    "    channel_coord=1,\n",
    "    transform_key=fusion_transform_key,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
